<?xml version="1.0" encoding="utf-8"?><?xml-stylesheet type="text/xsl" href="https://merryldt.github.io/rss.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/">
  <channel>
    <atom:link href="https://merryldt.github.io/rss.xml" rel="self" type="application/rss+xml"/>
    <title>魔力社区</title>
    <link>https://merryldt.github.io/</link>
    <description>系统的梳理,可以让人思路清晰</description>
    <language>zh-CN</language>
    <pubDate>Sat, 15 Jul 2023 09:16:23 GMT</pubDate>
    <lastBuildDate>Sat, 15 Jul 2023 09:16:23 GMT</lastBuildDate>
    <generator>vuepress-plugin-feed2</generator>
    <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
    <category>JVM</category>
    <item>
      <title>GC 体系总结</title>
      <link>https://merryldt.github.io/moyu/jvm/01_total.html</link>
      <guid>https://merryldt.github.io/moyu/jvm/01_total.html</guid>
      <source url="https://merryldt.github.io/rss.xml">GC 体系总结</source>
      <description>背景 基础知识介绍 Mutator 应用程序,即垃圾生产者； TLAB Thread Local Allocation Buffer 的缩写，基于CAS 的独享线程(Mutator Threads) 可以优先将对象分配在Eden 中的一块内存，因为是Java 线程独享的内存区没有锁竞争，所以分配速度更快，每个TLAB 都是一个线程独享的。 Card Table 卡表,主要是用来标记卡页的状态，每个卡表项对应一个卡页。当卡页中一个对象引用有写操作时，写屏障将会标记对象所在的卡表状态改为dirty,卡表的本质是用来解决跨代引用的问题。 https://stackoverflow.com/questions/19154607/how-actually-card-table-and-writer-barrier-works SLA（服务等级协议): Service-Level Agreement的缩写；指的是系统服务提供者（Provider）对客户（Customer）的⼀个服务承诺。这是衡量⼀个⼤型分布式系统是否“健康”的常见⽅法。 可⽤性、准确性、系统容量和延迟代完善09999</description>
      <category>JVM</category>
      <pubDate>Sat, 15 Jul 2023 09:14:38 GMT</pubDate>
      <content:encoded><![CDATA[<h1> 背景</h1>
<h1> 基础知识介绍</h1>
<ol>
<li>Mutator<br>
应用程序,即垃圾生产者；</li>
<li>TLAB<br>
Thread Local Allocation Buffer 的缩写，基于CAS 的独享线程(Mutator Threads) 可以优先将对象分配在Eden 中的一块内存，因为是Java 线程独享的内存区没有锁竞争，所以分配速度更快，每个TLAB 都是一个线程独享的。</li>
<li>Card Table<br>
卡表,主要是用来标记卡页的状态，每个卡表项对应一个卡页。当卡页中一个对象引用有写操作时，写屏障将会标记对象所在的卡表状态改为dirty,卡表的本质是用来解决跨代引用的问题。
<a href="https://stackoverflow.com/questions/19154607/how-actually-card-table-and-writer-barrier-works" target="_blank" rel="noopener noreferrer">https://stackoverflow.com/questions/19154607/how-actually-card-table-and-writer-barrier-works</a></li>
<li>SLA（服务等级协议):<br>
Service-Level Agreement的缩写；指的是系统服务提供者（Provider）对客户（Customer）的⼀个服务承诺。这是衡量⼀个⼤型分布式系统是否“健康”的常见⽅法。<br>
可⽤性、准确性、系统容量和延迟<a href="/moyu/jvm/09999_SLA.html" target="blank">代完善09999</a></li>
</ol>
<h1> 路径</h1>
<figure><figcaption>Alt text</figcaption></figure>
<h2> 1. 建立知识体系</h2>
<blockquote>
<p>学习GC 的基础知识,包括:</p>
</blockquote>
<ol>
<li><a href="/moyu/jvm/02_jvm_MemoryStructure.html" target="blank">jvm的内存结构和对象分配</a></li>
<li><a href="/moyu/jvm/03_GarbageCollectionMethod.html" target="blank">垃圾收集方法</a></li>
<li><a href="/moyu/jvm/04_GarbageCollector.html" target="blank">垃圾收集器</a></li>
<li><a href="/moyu/jvm/05_GCAnalysisTools.html" target="blank">掌握一些常用的GC问题分析工具</a></li>
</ol>
<h2> 2. 确定评价指标</h2>
<ol>
<li><a href="/moyu/jvm/06_GCEvaluationCriteria.html" target="blank">了解基本 GC 的评价方法</a></li>
<li>摸清如何设定独立系统的指标;</li>
<li>在业务场景中判断 GC 是否存在问题的手段;</li>
</ol>
<h2> 3. 场景调优实践</h2>
<ol>
<li><a href="/moyu/jvm/09_9CMS.html" target="blank">分析与解决九种 CMS 中常见 GC 问题场景</a></li>
</ol>
<h2> 4. 总结优化经验</h2>
]]></content:encoded>
    </item>
    <item>
      <title>基本 GC 的评价方法</title>
      <link>https://merryldt.github.io/moyu/jvm/06_GCEvaluationCriteria.html</link>
      <guid>https://merryldt.github.io/moyu/jvm/06_GCEvaluationCriteria.html</guid>
      <source url="https://merryldt.github.io/rss.xml">基本 GC 的评价方法</source>
      <description>GC的两个核心指标 延迟(Latency): 最大停顿时间，即垃圾收集过程中一次STW的最长时间，越短越好，一定程度上可以接受频次的增大. 吞吐量(Throughput): 应用系统的生命周期内，由于GC线程会占用应用程序当前可用的CPU 时钟周期，吞吐量即为应用程序有效花费的时间占系统总运行时间的百分比。 例如： 系统运行了100min,GC耗时1min,则系统吞吐量为99%,吞吐量优先的收集器可以接受较长的停顿。 p99 图示 p99 即为一次停顿的时间不超过应用服务的 TP9999，GC 的吞吐量不小于 99.99%。</description>
      <category>JVM</category>
      <pubDate>Sat, 15 Jul 2023 09:14:38 GMT</pubDate>
      <content:encoded><![CDATA[<h1> GC的两个核心指标</h1>
<h2> 延迟(Latency):</h2>
<p>最大停顿时间，即垃圾收集过程中一次STW的最长时间，越短越好，一定程度上可以接受频次的增大.</p>
<h3> 吞吐量(Throughput):</h3>
<p>应用系统的生命周期内，由于GC线程会占用应用程序当前可用的CPU 时钟周期，吞吐量即为应用程序有效花费的时间占系统总运行时间的百分比。<br>
例如： 系统运行了100min,GC耗时1min,则系统吞吐量为99%,吞吐量优先的收集器可以接受较长的停顿。</p>
<h1> p99 图示</h1>
<p>
p99 即为<strong>一次停顿的时间不超过应用服务的 TP9999，GC 的吞吐量不小于 99.99%</strong>。</p>
<h2> 例子：</h2>
<p>假设某个服务 A 的 TP9999 为 80 ms，平均 GC 停顿为 30 ms，那么该服务的最大停顿时间最好不要超过 80 ms，GC 频次控制在 5 min 以上一次。如果满足不了，那就需要调优或者通过更多资源来进行并联冗余。<br>
1/10000 = (1(次数)* x (平均GC停顿时间)) /  y(总时间)<br>
5min 等于 5<em>60</em>1000 = 300000毫秒；30 /300000= 1/10000</p>
<h1> 引入</h1>
<p><a href="/moyu/jvm/09002_ServerPerformanceParameters.html" target="blank">服务器的性能参数</a></p>
]]></content:encoded>
    </item>
    <item>
      <title></title>
      <link>https://merryldt.github.io/moyu/jvm/09001_SLA.html</link>
      <guid>https://merryldt.github.io/moyu/jvm/09001_SLA.html</guid>
      <source url="https://merryldt.github.io/rss.xml"></source>
      <description>可⽤性（Ava ila bilty） 可⽤性指的是系统服务能正常运⾏所占的时间百分⽐。 如果我们搭建了⼀个拥有“100％可⽤性”的系统服务，那就意味着这个系统在任何时候都能正常运⾏。是不是很完美？但真要实现这样的⽬标其实⾮常困难，并且成本也会很⾼。 我们知道，即便是⼤名⿍⿍的亚马逊AWS云计算服务这样⼤型的、对⽤户来说极为关键的系统，也不能承诺100％的可⽤性，它的系统服务从推出到现在，也有过服务中断（Service Outage）的时候。 对于许多系统⽽⾔，四个9的可⽤性（99.99％ Availability，或每年约50分钟的系统中断时间）即可以被认为是⾼可⽤性（High availability）。 说到这⾥，我来为你揭开⼀开始所提到的“99.9% Availability”的真实含义。 “99.9% Availability”指的是⼀天当中系统服务将会有⼤约86秒的服务间断期。服务间断也许是因为系统维护，也有可能是因为系统在更新升级系统服务。 86秒这个数字是怎么算出来的呢？ 99.9%意味着有0.1%的可能性系统服务会被中断，⽽⼀天中有24⼩时 × 60分钟 × 60秒，也就是有(24 × 60 × 60 × 0.001) = 86.4秒的可能系统服务被中断了。⽽上⾯所说的四个9的⾼可⽤性服务就是承诺可以将⼀天当中的服务中断时间缩短到只有(24 × 60 × 60 × 0.0001) = 8.64秒。 准确性（Ac c ur a c y） 准确性指的是我们所设计的系统服务中，是否允许某些数据是不准确的或者是丢失了的。如果允许这样的情况发⽣，⽤户可以接受的概率（百分⽐）是多少？ 这该怎么衡量呢？不同的系统平台可能会⽤不同的指标去定义准确性。很多时候，系统架构会以错误率（Error Rate）来定义这⼀项SLA。 怎么计算错误率呢？可以⽤导致系统产⽣内部错误（Internal Error）的有效请求数，除以这期间的有效请求总数。 image.png 例如，我们在⼀分钟内发送100个有效请求到系统中，其中有5个请求导致系统返回内部错误，那我们可以说这⼀分钟系统的错误率是 5 / 100 = 5%。 下⾯，我想带你看看硅⾕⼀线公司所搭建的架构平台的准确性SLA。 Google Cloud Platform的SLA中，有着这样的准确性定义：每个⽉系统的错误率超过5%的时间要少于0.1%，以每分钟为单位来计算。 ⽽亚马逊AWS云计算平台有着稍微不⼀样的准确性定义：以每5分钟为单位，错误率不会超过0.1%。 你看，我们可以⽤错误率来定义准确性，但具体该如何评估系统的准确性呢？⼀般来说，我们可以采⽤性能测试（Performance Test）或者是查看系统⽇志（Log）两种⽅法来评估。 具体的做法我会在后⾯展开讲解，今天你先理解这项指标就可以了。 系统容量（Ca pa c ity） 在数据处理中，系统容量通常指的是系统能够⽀持的预期负载量是多少，⼀般会以每秒的请求数为单位来表⽰。 我们常常可以看见，某个系统的架构可以处理的QPS （Queries Per Second）是多少⼜或者RPS（Requests Per Second）是多少。这⾥的QPS或者是RPS就是指系统每秒可以响应多少请求数。 我们来看看之前Twitter发布的⼀项数据，Twitter系统可以响应30万的QPS来读取Twitter Timelines。这⾥Twitter系统给出的就是他们对于系统容量（Capacity）的SLA。 你可能会问，我要怎么给⾃⼰设计的系统架构定义出准确的QPS呢？以我的经验看，可以有下⾯这⼏种⽅式。 第⼀种，是使⽤限流（Throttling）的⽅式。 如果你是使⽤Java语⾔进⾏编程的，就可以使⽤Google Guava库中的RateLimiter类来定义每秒最多发送多少请求到后台处理。 假设我们在每台服务器都定义了⼀个每秒最多处理1000个请求的RateLimiter，⽽我们有N台服务器，在最理想的情况下，我们的QPS可以达到1000 * N。 这⾥要注意的雷区是，这个请求数并不是设置得越多越好。因为每台服务器的内存有限，过多的请求堆积在服务器中有可能会导致内存溢出 （Out-Of-Memory）的异常发⽣，也就是所有请求所需要占⽤的内存超过了服务器能提供的内存，从⽽让整个服务器崩溃。 第⼆种，是在系统交付前进⾏性能测试（Performance Test）。 我们可以使⽤像Apache JMeter⼜或是LoadRunner这类型的⼯具对系统进⾏性能测试。这类⼯具可以测试出系统在峰值状态下可以应对的QPS 是多少。 当然了，这⾥也是有雷区的。 有的开发者可能使⽤同⼀类型的请求参数，导致后台服务器在多数情况下命中缓存（Cache Hit）。这个时候得到的QPS可能并不是真实的QPS。 打个⽐⽅，服务器处理请求的正常流程需要查询后台数据库，得到数据库结果后再返回给⽤户，这个过程平均需要1秒。在第⼀次拿到数据库结果后，这个数据就会被保存在缓存中，⽽如果后续的请求都使⽤同⼀类型的参数，导致结果不需要从数据库得到，⽽是直接从缓存中得到，这个过程我们假设只需要0.1秒。那这样，我们所计算出来的QPS就会⽐正常的⾼出10倍。所以在⽣成请求的时候，要格外注意这⼀点。 第三种，是分析系统在实际使⽤时产⽣的⽇志（Log）。 系统上线使⽤后，我们可以得到⽇志⽂件。⼀般的⽇志⽂件会记录每个时刻产⽣的请求。我们可以通过系统每天在最繁忙时刻所接收到的请求数， 来计算出系统可以承载的QPS。 不过，这种⽅法不⼀定可以得到系统可以承载的最⼤QPS。 在这⾥打个⽐喻，⼀家可以容纳上百桌客⼈的餐馆刚开业，因为客流量还⽐较⼩，在每天最繁忙的时候只接待了10桌客⼈。那我们可以说这家餐馆最多只能接待10桌客⼈吗？不可以。 同样的，以分析系统⽇志的⽅法计算出来的QPS并不⼀定是服务器能够承载的最⼤QPS。想要得到系统能承受的最⼤QPS，更多的是性能测试和⽇志分析相结合的⼿段。 延迟（Latency） 延迟指的是系统在收到⽤户的请求到响应这个请求之间的时间间隔。 在定义延迟的SLA时，我们常常看到系统的SLA会有p95或者是p99这样的延迟声明。这⾥的p指的是percentile，也就是百分位的意思。如果说⼀个系统的p95 延迟是1秒的话，那就表⽰在100个请求⾥⾯有95个请求的响应时间会少于1秒，⽽剩下的5个请求响应时间会⼤于1秒。 下⾯我们⽤⼀个具体的例⼦来说明延迟这项指标在SLA中的重要性。 假设，我们已经设计好了⼀个社交软件的系统架构。这个社交软件在接收到⽤户的请求之后，需要读取数据库中的内容返回给⽤户。 为了降低系统的延迟，我们会将数据库中内容放进缓存（Cache）中，以此来减少数据库的读取时间。在系统运⾏了⼀段时间后，我们得到了⼀些缓存命中率（Cache Hit Ratio）的信息。有90%的请求命中了缓存，⽽剩下的10%的请求则需要重新从数据库中读取内容。 这时服务器所给我们的p95或者p99延迟恰恰就衡量了系统的最长时间，也就是从数据库中读取内容的时间。作为⼀个优秀架构师，你可以通过改进缓存策略从⽽提⾼缓存命中率，也可以通过优化数据库的Schema或者索引（Index）来降低p95或p99 延迟。 总⽽⾔之，当p95或者p99过⾼时，总会有5%或者1%的⽤户抱怨产品的⽤户体验太差，这都是我们要通过优化系统来避免的。</description>
      <pubDate>Sat, 15 Jul 2023 09:14:38 GMT</pubDate>
      <content:encoded><![CDATA[<ol>
<li>可⽤性（Ava ila bilty）
可⽤性指的是系统服务能正常运⾏所占的时间百分⽐。
如果我们搭建了⼀个拥有“100％可⽤性”的系统服务，那就意味着这个系统在任何时候都能正常运⾏。是不是很完美？但真要实现这样的⽬标其实⾮常困难，并且成本也会很⾼。
我们知道，即便是⼤名⿍⿍的亚马逊AWS云计算服务这样⼤型的、对⽤户来说极为关键的系统，也不能承诺100％的可⽤性，它的系统服务从推出到现在，也有过服务中断（Service Outage）的时候。
对于许多系统⽽⾔，四个9的可⽤性（99.99％ Availability，或每年约50分钟的系统中断时间）即可以被认为是⾼可⽤性（High availability）。
说到这⾥，我来为你揭开⼀开始所提到的“99.9% Availability”的真实含义。
“99.9% Availability”指的是⼀天当中系统服务将会有⼤约86秒的服务间断期。服务间断也许是因为系统维护，也有可能是因为系统在更新升级系统服务。
86秒这个数字是怎么算出来的呢？
99.9%意味着有0.1%的可能性系统服务会被中断，⽽⼀天中有24⼩时 × 60分钟 × 60秒，也就是有(24 × 60 × 60 × 0.001) = 86.4秒的可能系统服务被中断了。⽽上⾯所说的四个9的⾼可⽤性服务就是承诺可以将⼀天当中的服务中断时间缩短到只有(24 × 60 × 60 × 0.0001) = 8.64秒。</li>
<li>准确性（Ac c ur a c y）
准确性指的是我们所设计的系统服务中，是否允许某些数据是不准确的或者是丢失了的。如果允许这样的情况发⽣，⽤户可以接受的概率（百分⽐）是多少？
这该怎么衡量呢？不同的系统平台可能会⽤不同的指标去定义准确性。很多时候，系统架构会以错误率（Error Rate）来定义这⼀项SLA。
怎么计算错误率呢？可以⽤导致系统产⽣内部错误（Internal Error）的有效请求数，除以这期间的有效请求总数。
image.png
例如，我们在⼀分钟内发送100个有效请求到系统中，其中有5个请求导致系统返回内部错误，那我们可以说这⼀分钟系统的错误率是 5 / 100 = 5%。
下⾯，我想带你看看硅⾕⼀线公司所搭建的架构平台的准确性SLA。
Google Cloud Platform的SLA中，有着这样的准确性定义：每个⽉系统的错误率超过5%的时间要少于0.1%，以每分钟为单位来计算。
⽽亚马逊AWS云计算平台有着稍微不⼀样的准确性定义：以每5分钟为单位，错误率不会超过0.1%。
你看，我们可以⽤错误率来定义准确性，但具体该如何评估系统的准确性呢？⼀般来说，我们可以采⽤性能测试（Performance Test）或者是查看系统⽇志（Log）两种⽅法来评估。
具体的做法我会在后⾯展开讲解，今天你先理解这项指标就可以了。</li>
<li>系统容量（Ca pa c ity）
在数据处理中，系统容量通常指的是系统能够⽀持的预期负载量是多少，⼀般会以每秒的请求数为单位来表⽰。
我们常常可以看见，某个系统的架构可以处理的QPS （Queries Per Second）是多少⼜或者RPS（Requests Per Second）是多少。这⾥的QPS或者是RPS就是指系统每秒可以响应多少请求数。
我们来看看之前Twitter发布的⼀项数据，Twitter系统可以响应30万的QPS来读取Twitter Timelines。这⾥Twitter系统给出的就是他们对于系统容量（Capacity）的SLA。
你可能会问，我要怎么给⾃⼰设计的系统架构定义出准确的QPS呢？以我的经验看，可以有下⾯这⼏种⽅式。
第⼀种，是使⽤限流（Throttling）的⽅式。
如果你是使⽤Java语⾔进⾏编程的，就可以使⽤Google Guava库中的RateLimiter类来定义每秒最多发送多少请求到后台处理。
假设我们在每台服务器都定义了⼀个每秒最多处理1000个请求的RateLimiter，⽽我们有N台服务器，在最理想的情况下，我们的QPS可以达到1000 * N。
这⾥要注意的雷区是，这个请求数并不是设置得越多越好。因为每台服务器的内存有限，过多的请求堆积在服务器中有可能会导致内存溢出
（Out-Of-Memory）的异常发⽣，也就是所有请求所需要占⽤的内存超过了服务器能提供的内存，从⽽让整个服务器崩溃。
第⼆种，是在系统交付前进⾏性能测试（Performance Test）。
我们可以使⽤像Apache JMeter⼜或是LoadRunner这类型的⼯具对系统进⾏性能测试。这类⼯具可以测试出系统在峰值状态下可以应对的QPS 是多少。
当然了，这⾥也是有雷区的。
有的开发者可能使⽤同⼀类型的请求参数，导致后台服务器在多数情况下命中缓存（Cache Hit）。这个时候得到的QPS可能并不是真实的QPS。
打个⽐⽅，服务器处理请求的正常流程需要查询后台数据库，得到数据库结果后再返回给⽤户，这个过程平均需要1秒。在第⼀次拿到数据库结果后，这个数据就会被保存在缓存中，⽽如果后续的请求都使⽤同⼀类型的参数，导致结果不需要从数据库得到，⽽是直接从缓存中得到，这个过程我们假设只需要0.1秒。那这样，我们所计算出来的QPS就会⽐正常的⾼出10倍。所以在⽣成请求的时候，要格外注意这⼀点。
第三种，是分析系统在实际使⽤时产⽣的⽇志（Log）。
系统上线使⽤后，我们可以得到⽇志⽂件。⼀般的⽇志⽂件会记录每个时刻产⽣的请求。我们可以通过系统每天在最繁忙时刻所接收到的请求数，
来计算出系统可以承载的QPS。
不过，这种⽅法不⼀定可以得到系统可以承载的最⼤QPS。
在这⾥打个⽐喻，⼀家可以容纳上百桌客⼈的餐馆刚开业，因为客流量还⽐较⼩，在每天最繁忙的时候只接待了10桌客⼈。那我们可以说这家餐馆最多只能接待10桌客⼈吗？不可以。
同样的，以分析系统⽇志的⽅法计算出来的QPS并不⼀定是服务器能够承载的最⼤QPS。想要得到系统能承受的最⼤QPS，更多的是性能测试和⽇志分析相结合的⼿段。</li>
<li>延迟（Latency）
延迟指的是系统在收到⽤户的请求到响应这个请求之间的时间间隔。
在定义延迟的SLA时，我们常常看到系统的SLA会有p95或者是p99这样的延迟声明。这⾥的p指的是percentile，也就是百分位的意思。如果说⼀个系统的p95 延迟是1秒的话，那就表⽰在100个请求⾥⾯有95个请求的响应时间会少于1秒，⽽剩下的5个请求响应时间会⼤于1秒。
下⾯我们⽤⼀个具体的例⼦来说明延迟这项指标在SLA中的重要性。
假设，我们已经设计好了⼀个社交软件的系统架构。这个社交软件在接收到⽤户的请求之后，需要读取数据库中的内容返回给⽤户。
为了降低系统的延迟，我们会将数据库中内容放进缓存（Cache）中，以此来减少数据库的读取时间。在系统运⾏了⼀段时间后，我们得到了⼀些缓存命中率（Cache Hit Ratio）的信息。有90%的请求命中了缓存，⽽剩下的10%的请求则需要重新从数据库中读取内容。
这时服务器所给我们的p95或者p99延迟恰恰就衡量了系统的最长时间，也就是从数据库中读取内容的时间。作为⼀个优秀架构师，你可以通过改进缓存策略从⽽提⾼缓存命中率，也可以通过优化数据库的Schema或者索引（Index）来降低p95或p99 延迟。
总⽽⾔之，当p95或者p99过⾼时，总会有5%或者1%的⽤户抱怨产品的⽤户体验太差，这都是我们要通过优化系统来避免的。</li>
</ol>
]]></content:encoded>
    </item>
    <item>
      <title>服务器性能参数</title>
      <link>https://merryldt.github.io/moyu/jvm/09002_ServerPerformanceParameters.html</link>
      <guid>https://merryldt.github.io/moyu/jvm/09002_ServerPerformanceParameters.html</guid>
      <source url="https://merryldt.github.io/rss.xml">服务器性能参数</source>
      <description>QPS(Queries Per Second): 每秒查询率，每秒钟处理完请求的次数；指发出请求到服务器处理完成功返回结果，是一台服务器每秒能够响应的查询次数，是对一个特定的查询服务器在规定时间内所处理流量多少的衡量标准。例如在server中有个counter，每处理一个请求加1，1秒后counter=QPS。对于衡量单个接口服务的处理能力，用QPS比较多。 TPS(Transactions Per Second)： 事务数/秒，是软件测试结果的测量单位。每秒钟处理完的事务次数，一般TPS是对整个系统来讲的。一个应用系统1s能完成多少事务处理，一个事务在分布式处理中，可能会对应多个请求。</description>
      <category>JVM</category>
      <pubDate>Sat, 15 Jul 2023 09:14:38 GMT</pubDate>
      <content:encoded><![CDATA[<h1> QPS(Queries Per Second):</h1>
<p>每秒查询率，每秒钟处理完请求的次数；指发出请求到服务器处理完成功返回结果，是一台服务器每秒能够响应的查询次数，是对一个特定的查询服务器在规定时间内所处理流量多少的衡量标准。例如在server中有个counter，每处理一个请求加1，1秒后counter=QPS。对于衡量单个接口服务的处理能力，用QPS比较多。</p>
<h1> TPS(Transactions Per Second)：</h1>
<p>事务数/秒，是软件测试结果的测量单位。每秒钟处理完的事务次数，一般TPS是对整个系统来讲的。一个应用系统1s能完成多少事务处理，一个事务在分布式处理中，可能会对应多个请求。</p>
<p>一个事务是指一个客户机向服务器发送请求然后服务器做出反应的过程。客户机在发送请求时开始计时，收到服务器响应后结束计时，以此来计算使用的时间和完成的事务个数。</p>
<p>包括了</p>
<ol>
<li>
<p>用户请求服务器</p>
</li>
<li>
<p>服务器自己的内部处理</p>
</li>
<li>
<p>服务器返回给用户</p>
</li>
</ol>
<p>每秒能够完成N个这3个过程，TPS也就是N；</p>
<h1> TPS和QPS区别：</h1>
<p>对于一个页面的一次访问，形成一个Tps；但一次页面请求，可能产生多次对服务器的请求，服务器对这些请求，就可计入“Qps”之中。</p>
<p>例如：访问一个页面会请求服务器3次，一次放，产生一个“T”，产生3个“Q”</p>
<p>系统吞吐量：应用系统每秒钟最大能接受的用户访问量，或者每秒钟最大能处理的请求数。</p>
<p>一个系统的吞度量(承压能力)与request对CPU的消耗、外部接口、IO等等紧密关联。单个reqeust对CPU消耗越高，外部系统接口、IO影响速度越慢，系统吞吐能力越低，反之越高。</p>
<h1> 系统吞吐量几个重要参数：</h1>
<p>QPS(TPS)、并发数、响应时间</p>
<p>QPS(TPS)：每秒钟请求/事务数</p>
<p>并发数： 系统能同时处理的请求/事务数</p>
<p>RT(response time)：  响应时间，一般取平均响应时间，处理一次请求所需要的平均处理时间</p>
<h2> 计算关系：</h2>
<p>并发数 = QPS (TPS)* RT</p>
<p>QPS (TPS)=并发数/RT</p>
]]></content:encoded>
    </item>
  </channel>
</rss>